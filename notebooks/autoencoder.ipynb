{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"autoencoder.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP0IhrOow7D9ZucytOd1Adj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"m7TVRvTxXpln","colab_type":"text"},"source":["## Autoencoder in Tensorflow\n","Implementation of a symetrical deep autoencoder with weight-tying using keras"]},{"cell_type":"code","metadata":{"id":"fdHt7L5iMMZD","colab_type":"code","colab":{}},"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qtu1h1-Pgmj4","colab_type":"text"},"source":["## AutoencoderBuilder\n","The builder class abstracts away the process of building a multi-layered, symmetrical tied-weight autoencoder"]},{"cell_type":"code","metadata":{"id":"6XJZ11R6HSPa","colab_type":"code","colab":{}},"source":["class AutoencoderBuilder():\n","  \"\"\"\n","  Builder for a multi-layer, symetrical autoencoder with weight-tying\n","  \"\"\"\n","  def __init__(self, input_shape, encoder_layer_sizes, activation='relu'):\n","    \"\"\"\n","    The decoder layer sizes will be symetrical of the encoder. The last layer\n","    of the encoding layer describes the bottle-neck layer or the latent \n","    representation of the input. Note: the input data must be normalized. \n","    \"\"\"\n","    self.input_shape = input_shape\n","    self.encoder_layer_sizes = encoder_layer_sizes\n","    self.activation = activation\n","    \n","  def build(self):\n","    \"\"\"Builds the autoencoder\"\"\"\n","    encoder_layers = self._encoder()\n","    decoder_layers = self._decoder(encoder_layers)\n","    return keras.models.Sequential(encoder_layers + decoder_layers)\n","\n","  def _encoder(self):\n","    \"\"\"Builds the encoding layers\"\"\"\n","    layers = []\n","    layers.append(keras.layers.Flatten(input_shape=(self.input_shape)))\n","    for size in self.encoder_layer_sizes:\n","      layers.append(keras.layers.Dense(size, activation=self.activation))\n","    return layers\n","\n","  def _decoder(self, encoder_layers):\n","    \"\"\"\n","    Builds the decoding layers by transposing encoder layers such\n","    that the encoder and decoder share weights\n","    \"\"\"\n","    layers = []\n","    for el in encoder_layers[:1:-1]:\n","      layers.append(DenseTranspose(el, activation=self.activation))\n","    layers.append(DenseTranspose(encoder_layers[1], activation='sigmoid'))\n","    layers.append(keras.layers.Reshape(self.input_shape))\n","    return layers"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LNDkU1ecg56k","colab_type":"text"},"source":["## Custom weight-tying layers"]},{"cell_type":"code","metadata":{"id":"2fVeprpcOTxP","colab_type":"code","colab":{}},"source":["# weight-tying dense (transpose) layer\n","# cite: https://mc.ai/a-beginners-guide-to-build-stacked-autoencoder-and-tying-weights-with-it/\n","\n","class DenseTranspose(keras.layers.Layer):\n","  def __init__(self, dense, activation=None, **kwargs):\n","    super().__init__(**kwargs)\n","    self.dense = dense\n","    self.activation = keras.activations.get(activation)\n","  \n","  def build(self, input_shape):\n","    super().build(input_shape)\n","    self.bias = self.add_weight(name=\"bias\",\n","                                shape=(self.dense.input_shape[-1]),\n","                                initializer=\"zeros\")\n","  \n","  def call(self, inputs):\n","    z = tf.matmul(inputs, self.dense.weights[0], transpose_b=True)\n","    return self.activation(z + self.bias)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vrH3ARXAhFc9","colab_type":"text"},"source":["## Testing on MNIST dataset"]},{"cell_type":"markdown","metadata":{"id":"hdhgtENphZfs","colab_type":"text"},"source":["#### MNIST data loading and preprocessing"]},{"cell_type":"code","metadata":{"id":"aviu0j3bhGce","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"outputId":"e125d430-a8d3-4241-a802-b9a3f014b97a","executionInfo":{"status":"ok","timestamp":1587688376350,"user_tz":240,"elapsed":1711,"user":{"displayName":"Jiawei Mai","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgzbgyoHCDQx1gLmC6pqbejFceklEUQnHTqpGoT=s64","userId":"04193727463512316924"}}},"source":["mnist = keras.datasets.mnist\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","# normalizing the data\n","x_train, x_test = x_train / 255.0, x_test /255.0"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hmlz_4cEhi3O","colab_type":"text"},"source":["#### Building an autoencoder"]},{"cell_type":"code","metadata":{"id":"Zj4WBnZWhnaU","colab_type":"code","outputId":"683b634b-7401-44b7-90bf-5c5ce79e33c9","executionInfo":{"status":"ok","timestamp":1587688389434,"user_tz":240,"elapsed":9036,"user":{"displayName":"Jiawei Mai","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgzbgyoHCDQx1gLmC6pqbejFceklEUQnHTqpGoT=s64","userId":"04193727463512316924"}},"colab":{"base_uri":"https://localhost:8080/","height":357}},"source":["ae_builder = AutoencoderBuilder(input_shape=(28,28),\n","                                encoder_layer_sizes=[392, 196],\n","                                activation='relu')\n","ae = ae_builder.build()\n","ae.summary()"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","flatten (Flatten)            (None, 784)               0         \n","_________________________________________________________________\n","dense (Dense)                (None, 392)               307720    \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 196)               77028     \n","_________________________________________________________________\n","dense_transpose (DenseTransp (None, 392)               77420     \n","_________________________________________________________________\n","dense_transpose_1 (DenseTran (None, 784)               308504    \n","_________________________________________________________________\n","reshape (Reshape)            (None, 28, 28)            0         \n","=================================================================\n","Total params: 385,924\n","Trainable params: 385,924\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"xKrFIE5Hi4DE","colab_type":"text"},"source":["#### Model compiling and traning"]},{"cell_type":"code","metadata":{"id":"u9wfRYJciHe0","colab_type":"code","outputId":"250ff2ea-c2fe-4f7a-902e-c0a3579a3060","executionInfo":{"status":"ok","timestamp":1587688473852,"user_tz":240,"elapsed":75098,"user":{"displayName":"Jiawei Mai","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgzbgyoHCDQx1gLmC6pqbejFceklEUQnHTqpGoT=s64","userId":"04193727463512316924"}},"colab":{"base_uri":"https://localhost:8080/","height":714}},"source":["ae.compile(optimizer='adam',\n","           loss='mean_squared_error',\n","           metrics=['accuracy'])\n","ae.fit(x_train, x_train, epochs=20)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Epoch 1/20\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.0106 - accuracy: 0.2846\n","Epoch 2/20\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.0038 - accuracy: 0.3240\n","Epoch 3/20\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.0030 - accuracy: 0.3341\n","Epoch 4/20\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.0025 - accuracy: 0.3397\n","Epoch 5/20\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.0023 - accuracy: 0.3437\n","Epoch 6/20\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.0022 - accuracy: 0.3469\n","Epoch 7/20\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.0020 - accuracy: 0.3493\n","Epoch 8/20\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.0019 - accuracy: 0.3515\n","Epoch 9/20\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.0018 - accuracy: 0.3532\n","Epoch 10/20\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.0017 - accuracy: 0.3547\n","Epoch 11/20\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.0017 - accuracy: 0.3561\n","Epoch 12/20\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.0016 - accuracy: 0.3577\n","Epoch 13/20\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.0016 - accuracy: 0.3579\n","Epoch 14/20\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.0016 - accuracy: 0.3588\n","Epoch 15/20\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.0015 - accuracy: 0.3595\n","Epoch 16/20\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.0015 - accuracy: 0.3603\n","Epoch 17/20\n","1875/1875 [==============================] - 3s 2ms/step - loss: 0.0015 - accuracy: 0.3604\n","Epoch 18/20\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.0015 - accuracy: 0.3608\n","Epoch 19/20\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.0014 - accuracy: 0.3614\n","Epoch 20/20\n","1875/1875 [==============================] - 4s 2ms/step - loss: 0.0014 - accuracy: 0.3620\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.keras.callbacks.History at 0x7fb9b0149b70>"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"uQLE7jAwk8tc","colab_type":"text"},"source":["### TODO: extract trained encoder for dimension reduction"]}]}