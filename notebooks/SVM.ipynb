{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SVM.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNS6MOQNVi7CvpSaOX74JeT"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"VkpDUYstCtLd","colab_type":"code","colab":{}},"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from PIL import Image\n","import io\n","from tempfile import mkdtemp\n","import pickle as pkl\n","import tarfile\n","import scipy\n","import time\n","\n","import operator as op\n","from functools import reduce\n","\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior() "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4bVU4T-2CuNQ","colab_type":"code","colab":{}},"source":["# read label file\n","tar_file = tarfile.open(\"/content/drive/My Drive/data_filelist_places365-standard.tar\")\n","f = tar_file.extractfile(\"places365_val.txt\")\n","content = f.read().decode(encoding=\"utf-8\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mTkPw3-nCnSJ","colab_type":"code","colab":{}},"source":["val_label = tf.convert_to_tensor(np.array([int(i.split(' ')[1]) for i in content.split('\\n')]), dtype = tf.float64)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dBc3BocECrSv","colab_type":"code","colab":{}},"source":["# input data_x and data_y should be numpy array\n","class multi_svm_model:\n","    def __init__(self, n_class, dimension, learning_rate=1e-2, regularization=1):\n","        self.learning_rate=learning_rate\n","        self.n_class=n_class\n","        self.dimension=dimension\n","        self.w=[]\n","        self.b=[]\n","        self.model_y=[]\n","        self.loss=[]\n","        self.hinge_loss=[]\n","        self.regularization_loss=[]\n","        #prediction=[]\n","        self.gt_y=[]\n","        self.lookup_class=dict()\n","        self.w_model=np.random.normal(loc=0,scale=1,size=(self.ncr(n_class,2), dimension))\n","        self.b_model=np.random.normal(loc=0,scale=1,size=(self.ncr(n_class,2),1))\n","        self.lookup_matrix=np.zeros((n_class, self.ncr(n_class,2)),dtype=np.float32)\n","        self.batch_x=tf.placeholder(tf.float32,shape=(None,dimension),name=\"batch_x\")\n","        self.batch_y=tf.placeholder(tf.float32,shape=(None,1),name=\"batch_y\")\n","        self.w = tf.Variable(tf.random_uniform([self.ncr(n_class,2), self.dimension]))\n","        self.b = tf.Variable(tf.random_uniform([self.ncr(n_class,2),1]))\n","        k=0\n","        for i in range(n_class-1):\n","            for j in range(i+1,n_class):\n","                self.lookup_class[k]=[i,j]\n","                k += 1\n","\n","        for i in range(n_class):\n","            for j in range(self.ncr(n_class,2)):\n","                if i in self.lookup_class[j]:\n","                    if self.lookup_class[j][0]==i:\n","                        self.lookup_matrix[i,j]=1.0\n","                    else:\n","                        self.lookup_matrix[i,j]=-1.0\n","\n","        for i in range(self.ncr(n_class,2)):\n","            # idx is the index of all the samples svm i concerns\n","            \n","            # idx=tf.where[condition is true] \n","            # tf.gather_nd(a,idx) \n","            # is equivalent to a[condition is true]\n","            idx=tf.where(tf.keras.backend.any(tf.equal(self.batch_y,self.lookup_class[i]),1)) \n","            \n","            # 1 x N matrix\n","            self.model_y.append(\n","                tf.tanh(\n","                    tf.matmul(tf.reshape(self.w[i,:],(1,dimension)),\n","                              tf.gather_nd(self.batch_x, idx), transpose_b=True)\n","                    + self.b[i,:])) \n","            \n","            # 1 x N matrix\n","            self.gt_y.append(tf.reshape(self.zonp(tf.cast(\n","                tf.equal(tf.gather_nd(self.batch_y, idx),self.lookup_class[i][0]),\n","                tf.float32)),(1,-1)))\n","            \n","            self.hinge_loss.append(tf.reduce_mean(tf.maximum(0.0,1-tf.multiply(self.model_y[i],self.gt_y[i]))))\n","            self.regularization_loss.append(regularization * tf.norm(self.w[i,:], 2))\n","            self.loss.append(self.hinge_loss[i] + self.regularization_loss[i])\n","        self.total_loss = sum(self.loss)\n","        self.opt = tf.train.RMSPropOptimizer(learning_rate).minimize(self.total_loss)\n","        \n","    def ncr(self,n, r):\n","        r = min(r, n-r)\n","        numer = reduce(op.mul, range(n, n-r, -1), 1)\n","        denom = reduce(op.mul, range(1, r+1), 1)\n","        return numer // denom\n","    \n","    def zonp(self,zero_one):\n","        return 2*zero_one-1\n","\n","    def fit(self,data_x,data_y,iter_time=100, batch_size=1000):\n","        with tf.Session() as sess:\n","            sess.run(tf.global_variables_initializer())\n","            sess.run([tf.assign(self.w,self.w_model),tf.assign(self.b,self.b_model)])\n","            \n","            total = data_y.shape[0]\n","            lower = 0\n","            upper = lower + batch_size\n","            \n","            counter = 0\n","            while counter < iter_time:\n","                lower = max(0,        (lower   +batch_size) % total)\n","                upper = min(total,      (lower +batch_size) % total)\n","                if lower<upper:\n","                    _,loss_val = sess.run([self.opt,self.total_loss], feed_dict={self.batch_x:data_x[lower:upper],self.batch_y:data_y[lower:upper]})\n","                    print(\"iteration:\"+str(counter)+\" loss:\"+str(loss_val))\n","                    counter += 1\n","                \n","            w_model,b_model = sess.run([self.w,self.b])\n","        self.w_model = w_model\n","        self.b_model = b_model\n","        \n","        return w_model,b_model\n","    \n","    def predict(self,data_x):\n","        result = np.argmax(np.matmul(self.lookup_matrix, np.tanh(np.matmul(self.w_model,data_x.T)) ),axis=0)\n","        return result.reshape(-1,1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q1TmxEViC1Ky","colab_type":"code","colab":{}},"source":["# X and y should be tensor\n","class SVM:\n","    def __init__(self, X, y, reg):\n","        \"\"\" Initialize the SVM attributes and initialize the weights vector to the zero vector. \n","            Attributes: \n","                X (tensor) : training data intputs\n","                y (tensor) : 1D numpy array of training data outputs\n","                reg (float) : regularizer parameter\n","                theta : 1D tensor of weights\n","        \"\"\"\n","        self.X = X\n","        self.y = y\n","        self.reg = reg\n","        self.theta = tf.zeros([self.X.shape[1], ], dtype = tf.float64)\n","    \n","    def objective(self, X, y):\n","        \"\"\" Calculate the objective value of the SVM. When given the training data (self.X, self.y), this is the \n","            actual objective being optimized. \n","            Args:\n","                X (array_like) : array of examples, where each row is an example\n","                y (array_like) : array of outputs for the training examples\n","            Output:\n","                (float) : objective value of the SVM when calculated on X,y\n","        \"\"\"\n","        theta_xy = tf.math.multiply(self.y, tf.tensordot(self.X, self.theta, axes = 1))\n","        hinge = tf.reduce_sum(tf.math.maximum(1-1-theta_xy, 0))\n","        reg = (self.reg / 2) * scipy.linalg.norm(self.theta) ** 2\n","        objective = hinge + reg\n","        \n","        return objective\n","    \n","    def gradient(self):\n","        \"\"\" Calculate the gradient of the objective value on the training examples. \n","            Output:\n","             (vector) : 1D numpy array containing the gradient\n","        \"\"\"\n","        u = tf.tensordot(self.X, self.theta, axes = 1)\n","        v = tf.math.multiply(self.y, u)\n","        v = tf.cast(tf.where(v<=1, 1., 0.), 'float64')\n","        y_v = (-self.y * v)[:,None]\n","        grad = tf.reshape((tf.transpose(y_v) @ self.X + self.theta * self.reg),[-1])\n","\n","        return grad\n","     \n","    def train(self, niters=100, learning_rate=1, verbose=False):\n","        \"\"\" Train the support vector machine with the given parameters. \n","            Args: \n","                niters (int) : the number of iterations of gradient descent to run\n","                learning_rate (float) : the learning rate (or step size) to use when training\n","                verbose (bool) : an optional parameter that you can use to print useful information (like objective value)\n","        \"\"\"\n","        for epoch in range(niters):\n","            grad = tf.transpose(self.gradient())\n","            self.theta = self.theta - (grad * learning_rate)\n","        return\n","\n","    def predict(self, X):\n","        \"\"\" Predict the class of each label in X. \n","            Args: \n","                X (array_like) : array of examples, where each row is an example\n","            Output:\n","                (vector) : 1D numpy array containing predicted labels\n","        \"\"\"\n","        predict = tf.tensordot(self.X, self.theta, axes = 1)\n","        # predict = tf.where(predict>0,1.,-1.)\n","        \n","        return predict"],"execution_count":0,"outputs":[]}]}