{"cells": [{"cell_type": "code", "execution_count": 132, "metadata": {}, "outputs": [], "source": "from google.cloud import storage\nimport tarfile\nimport numpy as np\nfrom scipy.spatial.distance import euclidean\nfrom scipy import exp\nfrom scipy.linalg import eigh\nfrom PIL import Image\nimport io\nimport time\nimport operator\nfrom pyspark.mllib.linalg import Vectors\nfrom pyspark.mllib.linalg.distributed import RowMatrix\nfrom matplotlib import pylab as plt"}, {"cell_type": "code", "execution_count": 2, "metadata": {}, "outputs": [], "source": "# test spark\ndata = [1,2,3,4]\nrdd = sc.parallelize(data)"}, {"cell_type": "code", "execution_count": 3, "metadata": {}, "outputs": [], "source": "bucket_name = \"dataproc-staging-us-central1-759291875656-wohgf1sk\"\nstorage_client = storage.Client()\nbucket = storage_client.bucket(bucket_name)"}, {"cell_type": "code", "execution_count": 4, "metadata": {}, "outputs": [], "source": "file_prefix = \"data/\"\nblobs = bucket.list_blobs(prefix=file_prefix, delimiter = '/')"}, {"cell_type": "code", "execution_count": 5, "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "data/\ndata/filelist_places365-standard.tar\ndata/test_data.tar\ndata/train_data.tar\ndata/val_data.tar\n"}], "source": "# list files in folder\nfor blob in blobs:\n    print(blob.name)"}, {"cell_type": "markdown", "metadata": {}, "source": "#### Download file from blob"}, {"cell_type": "code", "execution_count": 6, "metadata": {"scrolled": false}, "outputs": [], "source": "file_name = \"val_data.tar\"\nblob = bucket.get_blob(file_prefix + file_name)\nblob.download_to_filename(\"small_data.tar\")"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": "data = []\ntar_file = tarfile.open(\"small_data.tar\")\n\n\nfor member in tar_file.getmembers():\n\n    f = tar_file.extractfile(member)\n    if f is not None:\n        content = f.read()\n        data.append(content)\n\ntar_file.close()"}, {"cell_type": "code", "execution_count": 7, "metadata": {}, "outputs": [], "source": "# test small amount of images\nTEST_NUM_IMAGES = 100\n\ndata = []\ntar_file = tarfile.open(\"small_data.tar\")\n\nnum = 0\nfor member in tar_file.getmembers():\n    if num <= TEST_NUM_IMAGES:\n        f = tar_file.extractfile(member)\n        if f is not None:\n            content = f.read()\n            data.append(content)\n        num += 1\n    else:\n        break\ntar_file.close()"}, {"cell_type": "code", "execution_count": 8, "metadata": {"scrolled": true}, "outputs": [], "source": "rdd = sc.parallelize(data)"}, {"cell_type": "markdown", "metadata": {}, "source": "### Resize, convert and flatten image data into array"}, {"cell_type": "code", "execution_count": 104, "metadata": {}, "outputs": [], "source": "def convert_img(img_data, dim_h):\n    \"\"\" \n  Resize image so that it has height dim_h and flatten the image\n  Args:\n    img_data: (bytes) image data\n    dim_h: (int) desired height\n  Returns:\n    img: (np array) the resized and flattened image\n    \"\"\"\n    img = Image.open(io.BytesIO(img_data))\n    img = img.convert(mode = 'L') # convert to grey scale\n    img = img.resize((dim_h, dim_h), Image.ANTIALIAS)\n    im_flatten = np.reshape(np.array(img),(-1,))\n#     im_flatten = Vectors.dense(im_flatten)\n    \n    return im_flatten.tolist()"}, {"cell_type": "code", "execution_count": 179, "metadata": {}, "outputs": [], "source": "rdd_flatten = rdd.map(lambda x: convert_img(x, 64)).cache()"}, {"cell_type": "markdown", "metadata": {}, "source": "### Implement PCA"}, {"cell_type": "code", "execution_count": 106, "metadata": {}, "outputs": [], "source": "# use pca function provided by spark\n# maximum number of columns: 65,535\nmat = RowMatrix(rdd_flatten)\n\n# Principal components are stored in a local dense matrix.\npc = mat.computePrincipalComponents(10)"}, {"cell_type": "code", "execution_count": 107, "metadata": {}, "outputs": [{"data": {"text/plain": "DenseMatrix(4096, 10, [-0.0265, -0.0262, -0.0261, -0.0263, -0.0253, -0.0252, -0.0249, -0.0249, ..., 0.0051, 0.0016, 0.0013, 0.0004, 0.001, 0.0004, 0.0053, 0.009], 0)"}, "execution_count": 107, "metadata": {}, "output_type": "execute_result"}], "source": "pc"}, {"cell_type": "code", "execution_count": 108, "metadata": {}, "outputs": [], "source": "# Project the rows to the linear space spanned by the top 10 principal components.\nprojected = mat.multiply(pc)"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": 52, "metadata": {}, "outputs": [], "source": "def compute_covariance(data):\n    \"\"\"\n    Compute covariance matrix for given RDD.\n    Args:\n    data: (RDD of np arrays) RDD representing the data\n    Returns:\n    covmat: (np array) covariance matrix of the RDD\n    \"\"\"\n    n = data.count()\n    mean = data.mean()\n    data_0_mean = data.map(lambda m: m - mean)\n    covmat = (data_0_mean\n                    .map(lambda mat: np.outer(mat.T, mat))\n                    .reduce(lambda x,y: x+y)) / float(n)\n    return covmat"}, {"cell_type": "code", "execution_count": 15, "metadata": {}, "outputs": [], "source": "def pca(data, k=2):\n    \"\"\"\n    Computes the top `k` principal components, their corresponding PCA scores, and eigenvalues.\n    Args:\n    data: (RDD of np arrays) RDD representing the data.\n    k: (int) number of principal components to find\n    Returns:\n    (eigenvectors, scores, eigenvalues): (np.ndarray, RDD of np.ndarray, np.ndarray)\n    \"\"\"\n\n    # Compute covariance matrix\n    covmat = compute_covariance(data)\n\n    # Compute eigenvalues & eigenvectors\n    eig_vals, eig_vecs = eigh(covmat)\n\n    # Sort the eigenvectors based on their eigenvalues\n    inds = np.argsort(eig_vals)\n    inds = inds[::-1]\n\n    # Find the `k` principal components, `k` scores, and all eigenvalues\n    components = eig_vecs[:,inds[:k]]\n    eigenvalues = eig_vals[inds]\n    scores = data.map(lambda m: m.dot(components))\n\n    return (components, scores, eigenvalues)"}, {"cell_type": "code", "execution_count": 181, "metadata": {}, "outputs": [], "source": "# Run PCA on the actual data\ntop_comps, top_scores, top_eigenvals = pca(rdd_flatten.map(lambda x: Vectors.dense(x)), 10)"}, {"cell_type": "code", "execution_count": 182, "metadata": {}, "outputs": [{"data": {"text/plain": "(4096,)"}, "execution_count": 182, "metadata": {}, "output_type": "execute_result"}], "source": "top_eigenvals.shape"}, {"cell_type": "code", "execution_count": 183, "metadata": {}, "outputs": [{"data": {"text/plain": "[array([-5874.04451701,  3275.41228657,  1606.86812073,  -536.11883759,\n         -687.56409978,   -64.52112675,  -217.51302467,   432.69275909,\n          111.26081469,   226.90419953]),\n array([-7739.93590997,  1711.57899632,    56.73387892, -2114.27076937,\n        -1391.27538992,   248.32092075,   465.58113685,    99.99948625,\n         1027.39581905,   581.3017812 ]),\n array([-4955.50470509,  4228.33465538,  1806.05317013, -1495.06222458,\n        -1686.02874393,  -885.38693382,  -135.29897658,  1212.23213987,\n          -64.93972595,    34.14565319]),\n array([-7744.32929818,  1157.33397593,   -11.24212822,  -840.27731286,\n          293.00488427, -1324.63214265,  -862.38506288,   330.76414178,\n         1038.6267776 ,    28.46015111]),\n array([-7.17540601e+03,  3.10681140e+03,  3.42356090e+03,  2.40681560e+00,\n        -3.12504738e+02, -1.06664435e+03,  4.13124193e+02, -4.92624694e+02,\n         3.30710718e+02, -5.21468157e+01])]"}, "execution_count": 183, "metadata": {}, "output_type": "execute_result"}], "source": "top_scores.take(5)"}, {"cell_type": "code", "execution_count": 184, "metadata": {}, "outputs": [{"data": {"text/plain": "(4096, 10)"}, "execution_count": 184, "metadata": {}, "output_type": "execute_result"}], "source": "top_comps.shape"}, {"cell_type": "markdown", "metadata": {}, "source": "### Implement KPCA"}, {"cell_type": "code", "execution_count": 81, "metadata": {}, "outputs": [], "source": "def sqeuclidean(s1,s2):\n    return float(euclidean(s1,s2)**2)"}, {"cell_type": "code", "execution_count": 146, "metadata": {}, "outputs": [], "source": "# https://github.com/marty90/spark-distance-matrix/blob/master/spark_distance_matrix.py\n\nPARTITIONS_1=100\nPARTITIONS_2=1000\n\ndef compute_distance(data):\n    \"\"\"\n    Compute distance matrix for given RDD.\n    Args:\n    data: (RDD of np arrays) RDD representing the data\n    Returns:\n    dist: (np array) distance matrix of the RDD\n    \"\"\"\n    distance = sqeuclidean\n    points = data.collect()[0]\n\n    # Create Distributed Elements\n    # Get a vector like:\n    #\n    # ( 1 , [e1, e2, ... ])\n    # ( 2 , [e1, e2, ... ])\n    #      ....\n    points_rdd = sc.parallelize( range(len(points)), PARTITIONS_1)\n    \n    # Compute cartesian product, and filter only the upper triangle matrix\n    couples_rdd = points_rdd.cartesian(points_rdd).filter(lambda t: t[0] >= t[1]).coalesce(PARTITIONS_2)\n\n    # Compute distances for a batch of pairs\n    def calc_distance (tups, points):\n\n        for tup in tups:\n            i1, i2 = tup\n            p1 = points [i1]\n            p2 = points [i2]\n            d = distance(p1,p2)\n            \n            yield (i1, (i2, d))   \n            # Emit distances for the lower triangle matrix\n            if i1 != i2:\n                yield (i2, (i1, d)) \n   \n    couples_distance = couples_rdd.mapPartitions(lambda tups: calc_distance(tups, points) )\n    \n    # Create Distance Matrix\n    # Get a vector like:\n    #\n    # ( 1 , [d1, d2, ... ])\n    # ( 2 , [d1, d2, ... ])\n    #      ....\n    \n    def get_distance_vector(t):\n        i, dist_items = t\n        dist_items_sorted = sorted (dist_items, key=operator.itemgetter(0))\n        distances = [d for index, d in dist_items_sorted]\n        return (i, distances)\n    \n    distributed_distance = couples_distance.groupByKey().map(get_distance_vector)\n    \n    return distributed_distance.map(lambda x: x[1])"}, {"cell_type": "code", "execution_count": 173, "metadata": {}, "outputs": [], "source": "def kpca(data, gamma, n_components):\n    \"\"\"\n    Implementation of a RBF kernel PCA.\n\n    Arguments:\n        data: (RDD of np arrays) RDD representing the data\n        gamma: A free parameter (coefficient) for the RBF kernel.\n        n_components: The number of components to be returned.\n\n    Returns the k eigenvectors (alphas) that correspond to the k largest\n        eigenvalues (lambdas).\n\n    \"\"\"\n    # Calculating the squared Euclidean distances for every pair of points\n    # in the MxN dimensional dataset.\n    sq_dists = np.array(compute_distance(data).collect())\n\n    # Computing the MxM kernel matrix.\n    K = exp(-gamma * sq_dists)\n\n    # Centering the symmetric NxN kernel matrix.\n    N = K.shape[0]\n    one_n = np.ones((N,N)) / N\n    K_norm = K - one_n.dot(K) - K.dot(one_n) + one_n.dot(K).dot(one_n)\n\n    # Obtaining eigenvalues in descending order with corresponding\n    # eigenvectors from the symmetric matrix.\n    eigvals, eigvecs = eigh(K_norm)\n\n    # Obtaining the i eigenvectors (alphas) that corresponds to the i highest eigenvalues (lambdas).\n    alphas = np.column_stack((eigvecs[:,-i] for i in range(1,n_components+1)))\n    lambdas = [eigvals[-i] for i in range(1,n_components+1)]\n\n    return alphas, lambdas"}, {"cell_type": "code", "execution_count": 174, "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "/opt/conda/anaconda/bin/ipython:22: DeprecationWarning: scipy.exp is deprecated and will be removed in SciPy 2.0.0, use numpy.exp instead\n/opt/conda/anaconda/bin/ipython:34: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n"}], "source": "alphas, lambdas = kpca(rdd_flatten, 10, 5)"}, {"cell_type": "code", "execution_count": 176, "metadata": {}, "outputs": [{"data": {"text/plain": "(100, 5)"}, "execution_count": 176, "metadata": {}, "output_type": "execute_result"}], "source": "alphas.shape"}, {"cell_type": "code", "execution_count": 178, "metadata": {}, "outputs": [{"data": {"text/plain": "[5.768555561804704,\n 3.000136201335044,\n 3.0000817307766314,\n 3.0000000211358326,\n 3.0000000043937973]"}, "execution_count": 178, "metadata": {}, "output_type": "execute_result"}], "source": "lambdas"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": "# filename = 'val_flatten.pkl'\n# rdd_flatten.saveAsPickleFile('gs://'+ bucket_name + '/' + file_prefix + filename)"}, {"cell_type": "code", "execution_count": 16, "metadata": {}, "outputs": [], "source": "# pickle_rdd = sc.pickleFile('gs://'+ bucket_name + '/' + file_prefix + filename ).collect()\n# pickle_rdd"}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ""}], "metadata": {"kernelspec": {"display_name": "PySpark", "language": "python", "name": "pyspark"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.10"}}, "nbformat": 4, "nbformat_minor": 2}