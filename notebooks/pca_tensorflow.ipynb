{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: importing libraries\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tarfile\n",
    "import io\n",
    "import os\n",
    "import psutil\n",
    "import time\n",
    "from tensorflow import keras\n",
    "from google.cloud import storage\n",
    "from tensorflow.keras.models import Model \n",
    "from keras import backend as K\n",
    "from PIL import Image\n",
    "from matplotlib import pylab as plt\n",
    "\n",
    "print('Done: importing libraries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: loading training data\n",
      "Done: loading validation data\n"
     ]
    }
   ],
   "source": [
    "# preprocessed imgs are all 32x32\n",
    "img_size = (32, 32)\n",
    "pixels_per_img = np.prod(img_size)\n",
    "\n",
    "# load training data\n",
    "training_data_filename = 'train_img_array.pkl'\n",
    "train_data_size = 1803460\n",
    "full_train = np.memmap(training_data_filename, dtype='float32', mode='r+',\n",
    "               shape=(train_data_size, img_size[0], img_size[1]))\n",
    "\n",
    "print('Done: loading training data')\n",
    "\n",
    "# load validation data\n",
    "val_data_filename = 'val_img_array.pkl'\n",
    "val_data_size = 36500\n",
    "full_val = np.memmap(val_data_filename, dtype='float32', mode='r+',\n",
    "               shape=(val_data_size, img_size[0], img_size[1]))\n",
    "\n",
    "print('Done: loading validation data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that it loaded properly\n",
    "print('full train shape:', full_train.shape)\n",
    "plt.imshow(full_train[4], cmap='Greys')\n",
    "print('Done: test loading train data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that it loaded properly\n",
    "print('full val shape:', full_val.shape)\n",
    "plt.imshow(full_val[4], cmap='Greys')\n",
    "print('Done: test loading val data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data shape: (1803000, 32, 32)\n",
      "train_data shape (flattened): (1803000, 1024)\n",
      "val_data shape (flattened): (36500, 1024)\n",
      "Done: test loading data (minus last 460 entries)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# not using full training dataset because last 460 entries were not preprocessed\n",
    "train_data = full_train[:1803000]\n",
    "print('train_data shape:', train_data.shape)\n",
    "plt.imshow(train_data[100], cmap='Greys')\n",
    "\n",
    "# flatten every image into a 1D vector to facilitate matrix computations later\n",
    "train_data = train_data.flatten().reshape((len(train_data), pixels_per_img))\n",
    "print('train_data shape (flattened):', train_data.shape)\n",
    "\n",
    "# also flatten validation data\n",
    "val_data = full_val.flatten().reshape((len(full_val), pixels_per_img))\n",
    "print('val_data shape (flattened):', val_data.shape)\n",
    "\n",
    "# convert to tensor (takes a few seconds)\n",
    "# train_data_t = tf.convert_to_tensor(train_data, dtype = tf.float64)\n",
    "\n",
    "print('Done: test loading data (minus last 460 entries)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: setting up data subsets\n"
     ]
    }
   ],
   "source": [
    "# small training data subset for debugging purposes\n",
    "train_data_sub = tf.convert_to_tensor(train_data[:100], dtype=tf.float64)\n",
    "val_data = tf.convert_to_tensor(val_data, dtype=tf.float64)\n",
    "\n",
    "# val_data_sub = tf.convert_to_tensor(val_data[:100], dtype = tf.float64)\n",
    "\n",
    "# print('train data shape:', train_data.shape)\n",
    "# print('val data shape:', val_data.shape)\n",
    "\n",
    "# setting up different-sized datasets so we can test how the PCA scales (takes a few secs each)\n",
    "# note: I'm not going to assign them to variables here so we can save memory\n",
    "\n",
    "# train_data_10k = tf.convert_to_tensor(train_data[:10000], dtype = tf.float64)\n",
    "# train_data_50k = tf.convert_to_tensor(train_data[:50000], dtype = tf.float64)\n",
    "# train_data_100k = tf.convert_to_tensor(train_data[:100000], dtype = tf.float64)\n",
    "# train_data_200k = tf.convert_to_tensor(train_data[:200000], dtype = tf.float64)\n",
    "# train_data_500k = tf.convert_to_tensor(train_data[:500000], dtype = tf.float64)\n",
    "\n",
    "print('Done: setting up data subsets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement PCA\n",
    "\n",
    "References:\n",
    "* https://dev.to/mmithrakumar/principal-components-analysis-with-tensorflow-2-0-21hl\n",
    "* https://towardsdatascience.com/principal-component-analysis-in-depth-understanding-through-image-visualization-892922f77d9f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: define normalize()\n"
     ]
    }
   ],
   "source": [
    "def normalize(data):\n",
    "    \"\"\" Normalize the data \n",
    "    Args:\n",
    "        data (tf.tensor)\n",
    "    Returns:    \n",
    "        norm_data (tf.tensor): normalized data\n",
    "    \"\"\"\n",
    "    \n",
    "    norm_data = tf.identity(data)\n",
    "    norm_data -= tf.reduce_mean(norm_data, axis=0)\n",
    "    return norm_data\n",
    "\n",
    "print('Done: define normalize()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests ---\n",
    "\n",
    "normalized_data = normalize(train_data_sub)\n",
    "\n",
    "print('normalized data shape:', normalized_data.shape)\n",
    "\n",
    "# display samples in first 2 dimensions\n",
    "plt.plot(normalized_data[:,0], normalized_data[:,1], '+', color='b')\n",
    "plt.grid()\n",
    "\n",
    "print('Done: test normalized data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance_matrix(data):\n",
    "    \"\"\" Compute covariance matrix of data \n",
    "    Args:\n",
    "        data (tf.tensor)\n",
    "    Returns:\n",
    "        covmat (tf.tensor): covariance matrix of data\n",
    "    \"\"\"\n",
    "    covmat = tf.tensordot(tf.transpose(data), data, axes=1)\n",
    "    return covmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests ---\n",
    "\n",
    "print('test covariance matrix shape:', covariance_matrix(normalized_data).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: define pca() and pca_projection()\n"
     ]
    }
   ],
   "source": [
    "def pca(data):\n",
    "    \"\"\" Perform PCA on the data to find principal components\n",
    "    Args:\n",
    "        data (tf.tensor): the data to find the principal components of\n",
    "    Returns:\n",
    "        eigen_values (tf.tensor), eigen_vectors (tf.tensor): the principal components\n",
    "    \"\"\"\n",
    "    \n",
    "    # normalize data\n",
    "    norm_data = normalize(data)\n",
    "    \n",
    "    # compute covariance matrix\n",
    "    covmat = covariance_matrix(norm_data)\n",
    "    \n",
    "    # compute eigenvalues & eigenvectors\n",
    "    eigvals, eigvecs = tf.linalg.eigh(covmat)\n",
    "    \n",
    "    # Sort the eigenvectors based on their eigenvalues; from highest to lowest\n",
    "    # inds = tf.argsort(eigvals, axis=-1, direction='DESCENDING')\n",
    "\n",
    "    # Find the top `k` principal components and all eigenvalues\n",
    "    # components = eigvecs[:,inds[:k]]  # how to do this with tensors???\n",
    "    # eigenvalues = eigvals[inds]\n",
    "    \n",
    "    return eigvals, eigvecs\n",
    "\n",
    "\n",
    "def pca_projection(eigvecs, data):\n",
    "    \"\"\" Compute projection of data using principal components \n",
    "    Args:\n",
    "        eigvecs: the principal components to project\n",
    "    \n",
    "    \"\"\"\n",
    "    proj = tf.tensordot(tf.transpose(eigvecs), tf.transpose(data), axes=1)\n",
    "\n",
    "    return proj\n",
    "    \n",
    "print('Done: define pca() and pca_projection()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: define reconstruction_error()\n"
     ]
    }
   ],
   "source": [
    "def reconstruction_error(original, proj):\n",
    "    \"\"\" Compute the reconstruction error between the original data and the projection \n",
    "    Args:\n",
    "        original: the original dataset\n",
    "        proj: the pca projection of the data\n",
    "    Returns: scalar representing RMSE of the original vs projection\n",
    "    \"\"\"\n",
    "    return tf.reduce_mean((original - proj)**2)\n",
    "\n",
    "print('Done: define reconstruction_error()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests ---\n",
    "\n",
    "# garbage collection to free up memory\n",
    "normalized_data = None\n",
    "\n",
    "eigen_values, eigen_vectors = pca(train_data_sub)\n",
    "projection = pca_projection(eigen_vectors, val_data)\n",
    "\n",
    "print('eigen_values shape:', eigen_values.shape)\n",
    "print('eigen_vectors shape:', eigen_vectors.shape)\n",
    "print('projection shape:', projection.shape)\n",
    "    \n",
    "print('Done: project data using principle components')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display projection (in 2D for this example)\n",
    "plt.plot(projection[0,:], projection[1,:], '+', color='b')\n",
    "plt.grid()\n",
    "\n",
    "# garbage collection\n",
    "train_data_sub = None\n",
    "projection = None\n",
    "eigen_values = None\n",
    "eigen_vectors = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test PCA on various data subsets\n",
    "\n",
    "Get the runtime, memory usage, and reconstruction (projection) error for each of the following dataset sizes:\n",
    "1. 10000\n",
    "2. 50000\n",
    "3. 100000\n",
    "4. 200000\n",
    "5. 500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done: define pca_test()\n"
     ]
    }
   ],
   "source": [
    "def pca_test(data, validation):\n",
    "    \"\"\" Run PCA and record metrics \n",
    "    \n",
    "    Returns the eigen_values, eigen_vectors, projection\n",
    "    \"\"\"\n",
    "    \n",
    "    # runtime: start time\n",
    "    start_time = time.time()\n",
    "    # memory usage: memory in use before running pca\n",
    "    pid = os.getpid()\n",
    "    ps = psutil.Process(pid)\n",
    "    before_memory = ps.memory_info()\n",
    "\n",
    "    eigen_values, eigen_vectors = pca(data)\n",
    "    projection = pca_projection(eigen_vectors, validation)\n",
    "#     projection = pca_projection(eigen_vectors, data)\n",
    "    \n",
    "#     recon_err = reconstruction_error(data, tf.transpose(projection))\n",
    "    recon_err = reconstruction_error(validation, tf.transpose(projection))\n",
    "    \n",
    "    # runtime: end time\n",
    "    end_time = time.time()\n",
    "    pca_time = end_time - start_time\n",
    "    # memory usage: memory used after running pca\n",
    "    after_memory = ps.memory_info()\n",
    "    \n",
    "    # print metrics\n",
    "    print('METRICS for %d subset:' %(len(data)))\n",
    "    print('pca runtime:', pca_time)\n",
    "    print(f'rss: {after_memory.rss - before_memory.rss} bytes')\n",
    "    print(f'vms: {after_memory.vms - before_memory.vms} bytes')\n",
    "    \n",
    "    print('reconstruction error:', recon_err)\n",
    "    \n",
    "    # display projection (in 2D for this example)\n",
    "    plt.plot(projection[0,:], projection[1,:], '+', color='b')\n",
    "    plt.grid()\n",
    "    \n",
    "    return eigen_values, eigen_vectors, projection\n",
    "    \n",
    "\n",
    "print('Done: define pca_test()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 10k\n",
    "pca_test(tf.convert_to_tensor(train_data[:10000], dtype = tf.float64), val_data)\n",
    "print('Done: test pca on 10k data subset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 50k\n",
    "pca_test(tf.convert_to_tensor(train_data[:50000], dtype = tf.float64), val_data)\n",
    "print('Done: test pca on 50k data subset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 100k\n",
    "pca_test(tf.convert_to_tensor(train_data[:100000], dtype = tf.float64), val_data)\n",
    "print('Done: test pca on 100k data subset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 200k\n",
    "pca_test(tf.convert_to_tensor(train_data[:200000], dtype = tf.float64), val_data)\n",
    "print('Done: test pca on 200k data subset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run Identity: Dst tensor is not initialized. [Op:Identity]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-3199eaf3597a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# test 500k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0meigen_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meigen_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprojection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Done: test pca on 500k data subset'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-fc4f161fa17f>\u001b[0m in \u001b[0;36mpca_test\u001b[0;34m(data, validation)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mbefore_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0meigen_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meigen_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprojection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca_projection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meigen_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#     projection = pca_projection(eigen_vectors, data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-34d96bda7939>\u001b[0m in \u001b[0;36mpca\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# normalize data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mnorm_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# compute covariance matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-1e8de2edfe9a>\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \"\"\"\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mnorm_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mnorm_data\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnorm_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# variables. Variables have correct handle data when graph building.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m   \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m   \u001b[0;31m# Propagate handle data for happier shape inference for resource variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_handle_data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36midentity\u001b[0;34m(input, name)\u001b[0m\n\u001b[1;32m   3824\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3825\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3826\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3827\u001b[0m   \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3828\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6604\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6605\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6606\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6607\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run Identity: Dst tensor is not initialized. [Op:Identity]"
     ]
    }
   ],
   "source": [
    "# test 500k\n",
    "eigen_values, eigen_vectors, projection = pca_test(tf.convert_to_tensor(train_data[:500000], dtype = tf.float64), val_data)\n",
    "print('Done: test pca on 500k data subset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Projection Output\n",
    "\n",
    "Use np.memmap to store the projected validation data to the GCS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the validation data projection & principal components\n",
    "output_projection = np.memmap('pca_output.pkl', dtype='float32', mode='w+', shape=(36500, 128))\n",
    "output_projection = projection\n",
    "\n",
    "print('Done: storing to pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
